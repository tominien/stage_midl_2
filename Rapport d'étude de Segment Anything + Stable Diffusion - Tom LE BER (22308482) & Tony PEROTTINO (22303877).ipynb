{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tominien/stage_midl_2/blob/main/Rapport%20d'%C3%A9tude%20de%20Segment%20Anything%20%2B%20Stable%20Diffusion%20-%20Tom%20LE%20BER%20(22308482)%20%26%20Tony%20PEROTTINO%20(22303877).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQJfLpFQD7u3"
      },
      "source": [
        "# Rapport d'étude de Segment Anything + Stable Diffusion - Tom LE BER (22308482) & Tony PEROTTINO (22303877)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTncupPND-cZ"
      },
      "source": [
        "## I. Installation des bibliothèques"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Télécharger NLF"
      ],
      "metadata": {
        "id": "uaf9wdJj3CeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A. Cloner le repo NLF (`https://github.com/isarandi/nlf.git`)"
      ],
      "metadata": {
        "id": "swpAcdlN3Kbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Cloner le repo NLF dans /content (repository officiel) :\n",
        "if not os.path.exists(\"/content/nlf\"):\n",
        "    %cd /content\n",
        "    !git clone https://github.com/isarandi/nlf.git"
      ],
      "metadata": {
        "id": "_NQ50cT_3FCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71cca931-2a56-491c-96e8-6fdffc3846f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'nlf'...\n",
            "remote: Enumerating objects: 304, done.\u001b[K\n",
            "remote: Counting objects: 100% (304/304), done.\u001b[K\n",
            "remote: Compressing objects: 100% (221/221), done.\u001b[K\n",
            "remote: Total 304 (delta 139), reused 237 (delta 82), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (304/304), 406.85 KiB | 20.34 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. Téléchargement du `TorchScript` associé"
      ],
      "metadata": {
        "id": "Qxov2hAO4XUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nlf\n",
        "\n",
        "# Créer le dossier pour le modèle et télécharger le TorchScript :\n",
        "!mkdir -p models\n",
        "!wget -q -O models/nlf_l_multi.torchscript https://bit.ly/nlf_l_pt # Le lien pointe vers le modèle TorchScript NLF-L publié dans les releases du repo."
      ],
      "metadata": {
        "id": "ccLHv3kS4Xo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497d8828-ccb2-4bd4-cbd4-a3f1e1812770"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Télécharger `Segment Anything (V2)`"
      ],
      "metadata": {
        "id": "5cdrX7xW5lWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "REPO_DIR = \"/content/sam2_repo\"\n",
        "\n",
        "%cd /content\n",
        "!rm -rf \"{REPO_DIR}\" 2>/dev/null\n",
        "!git clone https://github.com/facebookresearch/sam2.git \"{REPO_DIR}\"\n",
        "\n",
        "!{sys.executable} -m pip uninstall -y SAM-2 sam-2 sam2 2>/dev/null\n",
        "!{sys.executable} -m pip install -e \"{REPO_DIR}\" --config-settings editable_mode=compat"
      ],
      "metadata": {
        "id": "IE-xrdEv5tXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3710da6-2a66-4a76-fb13-d8f9f9d51d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into '/content/sam2_repo'...\n",
            "remote: Enumerating objects: 1070, done.\u001b[K\n",
            "remote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1070/1070), 128.11 MiB | 12.40 MiB/s, done.\n",
            "Resolving deltas: 100% (381/381), done.\n",
            "Obtaining file:///content/sam2_repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Télécharger `Stable Diffusion`"
      ],
      "metadata": {
        "id": "OPFKRZnz5wYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "NlAEfls96AIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "vDOqF6pS6A3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Vérification de l'environnement"
      ],
      "metadata": {
        "id": "V6E5Y9ONhXC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "\n",
        "print(f\"torch = {torch.__version__}\")\n",
        "print(f\"torchvision = {torchvision.__version__}\")\n",
        "print(f\"cuda = {torch.cuda.is_available()}\")\n",
        "print(f\"torch cuda = {torch.version.cuda}\")"
      ],
      "metadata": {
        "id": "Vk4pkeVnhZw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Chargement des données et import des bibliothèques"
      ],
      "metadata": {
        "id": "SKaf71QZ2lzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Montage du Google Drive et extraction des images"
      ],
      "metadata": {
        "id": "d3T96LZp8NKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A. Montage du Google Drive"
      ],
      "metadata": {
        "id": "ZwNXirYV8aIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1TFWt5nc8k5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. Définition des chemins"
      ],
      "metadata": {
        "id": "-cKi0Dwx8nOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIRECTORY = \"/content/drive/MyDrive/02 - Cours/01 - MIDL/Stage MIDL\" # A remplacer par le chemin vers le dossier de votre Google Drive contenant la vidéo \"video.mp4\" et de dossier \"frames\" contenant toutes les frames de la vidéo.\n",
        "VIDEO_PATH = os.path.join(BASE_DIRECTORY, \"video.mp4\")\n",
        "FRAMES_DIRECTORY = os.path.join(BASE_DIRECTORY, \"frames\")\n",
        "\n",
        "# Créer le dossier \"frames\", s'il n'existe pas déjà :\n",
        "os.makedirs(FRAMES_DIRECTORY, exist_ok=True)"
      ],
      "metadata": {
        "id": "U3kNBNPK8qfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C. Extraction toutes les images de la vidéo"
      ],
      "metadata": {
        "id": "sPEWsoKh8s9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraire toutes les frames :\n",
        "if not (os.path.isdir(FRAMES_DIRECTORY) and os.listdir(FRAMES_DIRECTORY)):\n",
        "    !ffmpeg -i \"{VIDEO_PATH}\" \"{FRAMES_DIRECTORY}/frame_%04d.png\""
      ],
      "metadata": {
        "id": "h5rTRchz8wzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Import de `NLF`"
      ],
      "metadata": {
        "id": "NXJU0W2s6mgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette partie, nous appliquons le modèle NLF à la vidéo de pole-dance.  \n",
        "L’objectif est de :\n",
        "1. Charger le modèle NLF (version TorchScript) sur GPU.\n",
        "2. Définir une fonction d’inférence sur une image unique (frame).\n",
        "3. Appliquer NLF à un sous-ensemble de frames de la vidéo.\n",
        "4. Visualiser et analyser qualitativement les prédictions (joints 2D et 3D).\n",
        "\n",
        "Ensuite, chargeons la version TorchScript du modèle NLF (`nlf_l_multi.torchscript`), qui a été téléchargée dans le dossier `/content/nlf/models/` depuis les Releases du dépôt GitHub."
      ],
      "metadata": {
        "id": "r8bx1MLC7ne_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A. Initialisation des paramètres"
      ],
      "metadata": {
        "id": "OXUH31et7f2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "# Paramètres d'expérience pour NLF :\n",
        "MAX_FRAMES = 5000 # Nombre maximum de frames à traiter (pour l'étude).\n",
        "FRAME_STEP = 165 # 1 = toutes les frames, 2 = une frame sur deux, etc.\n",
        "\n",
        "# FRAMES_DIRECTORY et BASE_DIRECTORY sont supposés définis dans la partie précédente :\n",
        "frame_paths_all = sorted(glob.glob(os.path.join(FRAMES_DIRECTORY, \"frame_*.png\")))\n",
        "\n",
        "# Sous-échantillonnage éventuel :\n",
        "frame_paths = frame_paths_all[::FRAME_STEP]\n",
        "\n",
        "# Limitation au nombre maximum de frames :\n",
        "frame_paths = frame_paths[:MAX_FRAMES]\n",
        "\n",
        "print(f\"Nombre total de frames extraites : {len(frame_paths_all)}\")\n",
        "print(f\"Nombre de frames utilisées pour NLF : {len(frame_paths)}\")"
      ],
      "metadata": {
        "id": "BDnkyX8g7ZRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. Chargement du modèle"
      ],
      "metadata": {
        "id": "MR8Y0vxE7jf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nlf\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Sélection du \"device\" :\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device utilisé :\", device)\n",
        "\n",
        "# Chemin vers le modèle TorchScript (doit déjà avoir été téléchargé) :\n",
        "MODEL_PATH = \"models/nlf_l_multi.torchscript\"\n",
        "\n",
        "# Chargement du modèle :\n",
        "model = torch.jit.load(MODEL_PATH, map_location=device).eval()\n",
        "\n",
        "print(\"Modèle NLF chargé avec succès.\")"
      ],
      "metadata": {
        "id": "2MqB1NnX702m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Import de `Segment Anything (V2)`"
      ],
      "metadata": {
        "id": "9JaRx_Hn724H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "REPO_DIR = \"/content/sam2_repo\"\n",
        "\n",
        "# Optionnel (pour tester) :\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.insert(0, REPO_DIR)\n",
        "sys.modules.pop(\"sam2\", None)\n",
        "\n",
        "# Test des imports :\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
        "print(f\"Imports de SAM 2 fonctionnels.\") # Si on n'a pas le print, c'est qu'on a eu une erreur avant."
      ],
      "metadata": {
        "id": "kzjyFDfUwrp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Import de `Stable Diffusion`"
      ],
      "metadata": {
        "id": "AzkwCaWe7_GV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "hM07r1CM8CbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "Xq3x0vaK8Dsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Incx-rfNTnFB"
      },
      "source": [
        "## III. Exécution de `NLF`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD3DIOlxGzML"
      },
      "source": [
        "### 1. Détection d'une personne dans une frame donnée"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous définissons une fonction `run_nlf_on_image(img_path)` qui :\n",
        "1. Charge une image de frame depuis son chemin.\n",
        "2. La met au bon format pour NLF (batch de taille 1).\n",
        "3. Appelle `model.detect_smpl_batched(...)`.\n",
        "4. Récupère les joints 2D et 3D pour **la première personne** détectée.\n",
        "5. Retourne un dictionnaire Python sérialisable (listes plutôt que tenseurs).\n",
        "\n",
        "Nous considérons qu’il n’y a qu’une seule personne (la pole-danseuse) dans la scène."
      ],
      "metadata": {
        "id": "r2Mcw_VY9ABy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PDCPCiSG4VQ"
      },
      "outputs": [],
      "source": [
        "def run_nlf_on_image(img_path: str):\n",
        "    \"\"\"\n",
        "    Applique NLF à une image unique.\n",
        "    Retourne un dictionnaire contenant :\n",
        "      - frame : nom de fichier de la frame,\n",
        "      - has_person : booléen indiquant si une personne a été détectée,\n",
        "      - joints2d : liste [J, 2] des coordonnées 2D des joints (en pixels),\n",
        "      - joints3d : liste [J, 3] des coordonnées 3D des joints (repère caméra).\n",
        "    \"\"\"\n",
        "    # Charger l'image (C,H,W), uint8 :\n",
        "    image = torchvision.io.read_image(img_path).to(device)\n",
        "\n",
        "    # Ajouter la dimension batch : [1, C, H, W]\n",
        "    frame_batch = image.unsqueeze(0)\n",
        "\n",
        "    # Inférence NLF :\n",
        "    with torch.inference_mode():\n",
        "        pred = model.detect_smpl_batched(frame_batch)\n",
        "\n",
        "    # pred[\"joints2d\"] et pred[\"joints3d\"] sont des listes (une entrée par élément du batch) :\n",
        "    joints2d_batch = pred[\"joints2d\"][0]\n",
        "    joints3d_batch = pred[\"joints3d\"][0]\n",
        "\n",
        "    # Si le modèle renvoie [num_persons, J, 2] ou [num_persons, J, 3] :\n",
        "    if joints2d_batch.ndim == 3:\n",
        "        # Si aucune personne détectée :\n",
        "        if joints2d_batch.shape[0] == 0:\n",
        "            return {\n",
        "                \"frame\": os.path.basename(img_path),\n",
        "                \"has_person\": False,\n",
        "                \"joints2d\": None,\n",
        "                \"joints3d\": None,\n",
        "            }\n",
        "        # On prend la première personne :\n",
        "        j2d = joints2d_batch[0] # [J, 2]\n",
        "        j3d = joints3d_batch[0] # [J, 3]\n",
        "    else:\n",
        "        # Cas où le modèle renvoie directement [J, 2] / [J, 3] :\n",
        "        j2d = joints2d_batch\n",
        "        j3d = joints3d_batch\n",
        "\n",
        "    # Conversion en numpy, puis en listes Python :\n",
        "    j2d_np = j2d.detach().cpu().numpy()\n",
        "    j3d_np = j3d.detach().cpu().numpy()\n",
        "\n",
        "    result = {\n",
        "        \"frame\": os.path.basename(img_path),\n",
        "        \"has_person\": True,\n",
        "        \"joints2d\": j2d_np.tolist(), # [J, 2]\n",
        "        \"joints3d\": j3d_np.tolist(), # [J, 3]\n",
        "    }\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqPv_UOLG5KD"
      },
      "source": [
        "### 2. Inférence sur un lot de frames"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous appliquons maintenant `run_nlf_on_image` à un sous-ensemble de frames :\n",
        "- Les chemins de frames sont définis dans la cellule de paramètres (`frame_paths`).\n",
        "- Nous mesurons le **temps d'inférence** par frame.\n",
        "- Nous stockons les résultats dans une liste `results`, chaque entrée correspondant à une frame.\n",
        "\n",
        "À la fin, nous sauvegardons les résultats dans un fichier JSON situé dans `BASE_DIRECTORY`."
      ],
      "metadata": {
        "id": "Q2MNrBci9S34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-h5v5PGHATF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "results = []\n",
        "times = []\n",
        "\n",
        "print(f\"Lancement de NLF sur {len(frame_paths)} frames.\")\n",
        "\n",
        "for img_path in tqdm(frame_paths, desc=\"NLF inference\"):\n",
        "    t0 = time.perf_counter()\n",
        "    res = run_nlf_on_image(img_path)\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    res[\"infer_time\"] = t1 - t0 # Temps pour cette frame.\n",
        "    results.append(res)\n",
        "    times.append(t1 - t0)\n",
        "\n",
        "times = np.array(times)\n",
        "\n",
        "print(f\"Nombre total de résultats : {len(results)}\")\n",
        "print(f\"Temps moyen par frame : {times.mean()*1000:.1f} ms\")\n",
        "print(f\"FPS approximatif : {1.0 / times.mean():.2f}\")\n",
        "\n",
        "# Sauvegarde des résultats dans un JSON :\n",
        "output_path = os.path.join(BASE_DIRECTORY, \"nlf_results_subset.json\")\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(results, f)\n",
        "\n",
        "print(f\"Résultats sauvegardés dans : {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByI9aLTUHBOz"
      },
      "source": [
        "### 3. Visualisation qualitative de NLF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous proposons plusieurs visualisations :\n",
        "1. **Overlay 2D des joints sur une frame** (qualité instantanée de la pose).\n",
        "2. **Mosaïque de plusieurs frames** avec leurs joints 2D.\n",
        "3. **Évolution temporelle de la profondeur (Z) d’un joint donné**.\n",
        "4. **Visualisation 3D d’une pose** à partir de `joints3d`.\n",
        "\n",
        "Ces visualisations serviront de base à l’analyse qualitative de NLF dans la section suivante."
      ],
      "metadata": {
        "id": "U40_pef89W9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A. Overlay 2D sur une frame"
      ],
      "metadata": {
        "id": "Q0QNmKar9YPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RorizZG5HGv3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filtrer uniquement les frames où une personne a été détectée :\n",
        "valid_results = [r for r in results if r[\"has_person\"]]\n",
        "\n",
        "print(f\"Frames avec personne détectée : {len(valid_results)}\")\n",
        "\n",
        "if len(valid_results) == 0:\n",
        "    raise ValueError(\"Aucune personne détectée dans les frames sélectionnées.\")\n",
        "\n",
        "# On prend un exemple (par ex. la première) :\n",
        "sample = valid_results[0]\n",
        "frame_name = sample[\"frame\"]\n",
        "j2d = np.array(sample[\"joints2d\"]) # [J, 2]\n",
        "\n",
        "img_path = os.path.join(FRAMES_DIRECTORY, frame_name)\n",
        "img = torchvision.io.read_image(img_path).permute(1, 2, 0).cpu().numpy() # [H,W,C]\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(img)\n",
        "plt.scatter(j2d[:, 0], j2d[:, 1], s=10)\n",
        "plt.title(f\"Exemple de détection 2D : {frame_name}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0mH3AzDL6DN"
      },
      "source": [
        "#### B. Mosaïque de plusieurs frames avec joints 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH8YOeU8MLxa"
      },
      "outputs": [],
      "source": [
        "# Nombre d'exemples à afficher dans la mosaïque :\n",
        "num_examples = min(8, len(valid_results))\n",
        "\n",
        "# On prend des indices uniformément répartis dans la séquence des frames :\n",
        "indices = np.linspace(0, len(valid_results) - 1, num_examples, dtype=int)\n",
        "\n",
        "cols = 4\n",
        "rows = int(np.ceil(num_examples / cols))\n",
        "\n",
        "plt.figure(figsize=(4 * cols, 4 * rows))\n",
        "\n",
        "for i, idx in enumerate(indices, start=1):\n",
        "    res = valid_results[idx]\n",
        "    frame_name = res[\"frame\"]\n",
        "    j2d = np.array(res[\"joints2d\"])\n",
        "\n",
        "    img_path = os.path.join(FRAMES_DIRECTORY, frame_name)\n",
        "    img = torchvision.io.read_image(img_path).permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    ax = plt.subplot(rows, cols, i)\n",
        "    ax.imshow(img)\n",
        "    ax.scatter(j2d[:, 0], j2d[:, 1], s=5)\n",
        "    ax.set_title(frame_name)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dONrd1nMN8n"
      },
      "source": [
        "#### C. Évolution de la profondeur Z d’un joint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIkeRGl3MQq0"
      },
      "outputs": [],
      "source": [
        "# Indice du joint étudié :\n",
        "JOINT_INDEX = 0 # 0 = tête.\n",
        "\n",
        "z_vals = []\n",
        "frames_idx = []\n",
        "\n",
        "for i, res in enumerate(results):\n",
        "    if not res[\"has_person\"]:\n",
        "        continue\n",
        "    j3d = np.array(res[\"joints3d\"]) # [J, 3]\n",
        "    if j3d.shape[0] <= JOINT_INDEX:\n",
        "        continue\n",
        "    z_vals.append(j3d[JOINT_INDEX, 2])\n",
        "    frames_idx.append(i)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(frames_idx, z_vals, marker=\"o\", linestyle=\"-\", linewidth=1)\n",
        "plt.xlabel(\"Index de frame (dans la séquence traitée)\")\n",
        "plt.ylabel(\"Profondeur Z (unité NLF)\")\n",
        "plt.title(f\"Évolution de la profondeur Z pour le joint {JOINT_INDEX}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExSkDoNkMSzA"
      },
      "source": [
        "#### D. Visualisation 3D d’une pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaQ_H1SwMVyk"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D  # Affichage 3D.\n",
        "\n",
        "# On vérifie qu'on a bien des résultats valides :\n",
        "valid_results = [r for r in results if r[\"has_person\"]]\n",
        "print(f\"Frames avec personne détectée : {len(valid_results)}\")\n",
        "\n",
        "if len(valid_results) == 0:\n",
        "    raise ValueError(\"Aucune personne détectée dans les frames sélectionnées.\")\n",
        "\n",
        "# Choix de l'index de frame à visualiser :\n",
        "FRAME_INDEX_FOR_3D_VIEW = 0\n",
        "\n",
        "sample = valid_results[FRAME_INDEX_FOR_3D_VIEW]\n",
        "frame_name = sample[\"frame\"]\n",
        "j2d = np.array(sample[\"joints2d\"]) # [J, 2]\n",
        "j3d = np.array(sample[\"joints3d\"]) # [J, 3]\n",
        "\n",
        "# Chargement de l'image correspondante :\n",
        "img_path = os.path.join(FRAMES_DIRECTORY, frame_name)\n",
        "img = torchvision.io.read_image(img_path).permute(1, 2, 0).cpu().numpy() # [H,W,C]\n",
        "\n",
        "# Remapping des axes pour que la pose 3D \"colle\" mieux à l'image :\n",
        "    # X_vis : gauche/droite\n",
        "    # Y_vis : profondeur (distance caméra)\n",
        "    # Z_vis : hauteur (haut/bas) -> on prend -Y pour que le haut soit \"vers le haut\"\n",
        "X_vis = j3d[:, 0]\n",
        "Y_vis = j3d[:, 2]\n",
        "Z_vis = -j3d[:, 1]\n",
        "\n",
        "# Petite fonction pour avoir des axes isotropes (squelette non écrasé) :\n",
        "def set_axes_equal(ax):\n",
        "    x_limits = ax.get_xlim3d()\n",
        "    y_limits = ax.get_ylim3d()\n",
        "    z_limits = ax.get_zlim3d()\n",
        "\n",
        "    x_range = abs(x_limits[1] - x_limits[0])\n",
        "    y_range = abs(y_limits[1] - y_limits[0])\n",
        "    z_range = abs(z_limits[1] - z_limits[0])\n",
        "\n",
        "    max_range = max([x_range, y_range, z_range]) / 2.0\n",
        "\n",
        "    mid_x = np.mean(x_limits)\n",
        "    mid_y = np.mean(y_limits)\n",
        "    mid_z = np.mean(z_limits)\n",
        "\n",
        "    ax.set_xlim3d([mid_x - max_range, mid_x + max_range])\n",
        "    ax.set_ylim3d([mid_y - max_range, mid_y + max_range])\n",
        "    ax.set_zlim3d([mid_z - max_range, mid_z + max_range])\n",
        "\n",
        "# Affichage des figures combinées en 2 colonnes :\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "    # Figure 1 (image + joints 2D) :\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.imshow(img)\n",
        "ax1.scatter(j2d[:, 0], j2d[:, 1], s=10)\n",
        "ax1.set_title(f\"Pose 2D\\n{frame_name}\")\n",
        "ax1.axis(\"off\")\n",
        "    # Figure 2 (visualisation 3D) :\n",
        "ax2 = fig.add_subplot(1, 2, 2, projection=\"3d\")\n",
        "ax2.scatter(X_vis, Y_vis, Z_vis)\n",
        "ax2.set_xlabel(\"X\")\n",
        "ax2.set_ylabel(\"Y\")\n",
        "ax2.set_zlabel(\"Z\")\n",
        "ax2.set_title(f\"Pose 3D\\n{frame_name}\")\n",
        "ax2.view_init(elev=10, azim=-90) # Vue \"de face\" pour que ça ressemble à la projection de l'image.\n",
        "ax2.grid(True, alpha=0.3) # Grille moins prononcée.\n",
        "set_axes_equal(ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7w75650HHGf"
      },
      "source": [
        "## IV. Exécution de `Segment Anything (V2)`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "... *(Création de calques **qualitatifs** de la danseuse.)*"
      ],
      "metadata": {
        "id": "XnfE0lrG9pvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "6gQfizMq9qWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# --- Test SAM2 sur UNE frame (bbox -> masque) ---\n",
        "MODEL_ID = \"facebook/sam2.1-hiera-base-plus\"  # bon compromis vitesse/qualité (checkpoints SAM2.1)\n",
        "# (tu peux mettre \"...-large\" si tu veux plus quali, mais plus lourd)\n",
        "\n",
        "import glob, os\n",
        "\n",
        "# 1) choisir une image\n",
        "if \"FRAMES_DIRECTORY\" in globals():\n",
        "    candidates = sorted(glob.glob(os.path.join(FRAMES_DIRECTORY, \"*.png\")) + glob.glob(os.path.join(FRAMES_DIRECTORY, \"*.jpg\")))\n",
        "else:\n",
        "    candidates = sorted(glob.glob(\"/content/*.png\") + glob.glob(\"/content/*.jpg\"))\n",
        "\n",
        "assert len(candidates) > 0, \"Aucune image trouvée. Vérifie FRAMES_DIRECTORY ou mets une image dans /content.\"\n",
        "test_img_path = candidates[0]\n",
        "print(\"Image de test:\", test_img_path)\n",
        "\n",
        "bgr = cv2.imread(test_img_path)\n",
        "assert bgr is not None, \"Impossible de lire l'image.\"\n",
        "img = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "H, W = img.shape[:2]\n",
        "\n",
        "# 2) bbox depuis NLF si possible, sinon fallback\n",
        "bbox = None\n",
        "if \"run_nlf_on_image\" in globals():\n",
        "    try:\n",
        "        pred = run_nlf_on_image(test_img_path)\n",
        "        pts = np.array(pred.get(\"joints2d\", []), dtype=float)\n",
        "        if pts.ndim == 2 and pts.shape[0] > 0 and pts.shape[1] == 2:\n",
        "            x1, y1 = pts.min(axis=0)\n",
        "            x2, y2 = pts.max(axis=0)\n",
        "            pad = 0.10\n",
        "            bw, bh = (x2 - x1), (y2 - y1)\n",
        "            bbox = np.array([x1 - pad*bw, y1 - pad*bh, x2 + pad*bw, y2 + pad*bh], dtype=float)\n",
        "            print(\"BBox depuis NLF:\", bbox)\n",
        "    except Exception as e:\n",
        "        print(\"NLF bbox fallback:\", repr(e))\n",
        "\n",
        "if bbox is None:\n",
        "    bbox = np.array([0.25*W, 0.10*H, 0.75*W, 0.90*H], dtype=float)\n",
        "    print(\"BBox fallback (centrée):\", bbox)\n",
        "\n",
        "bbox[0] = np.clip(bbox[0], 0, W-1)\n",
        "bbox[2] = np.clip(bbox[2], 0, W-1)\n",
        "bbox[1] = np.clip(bbox[1], 0, H-1)\n",
        "bbox[3] = np.clip(bbox[3], 0, H-1)\n",
        "\n",
        "# 3) SAM2 predictor\n",
        "predictor = SAM2ImagePredictor.from_pretrained(MODEL_ID)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "predictor.model.to(device)\n",
        "\n",
        "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16, enabled=torch.cuda.is_available()):\n",
        "    predictor.set_image(img)\n",
        "    masks, scores, _ = predictor.predict(box=bbox, multimask_output=True)  # bbox XYXY en pixels\n",
        "\n",
        "best_idx = int(np.argmax(scores))\n",
        "best_mask = masks[best_idx].astype(bool)\n",
        "print(\"scores:\", scores, \"best:\", scores[best_idx])\n",
        "\n",
        "# 4) export calque RGBA\n",
        "rgba = np.dstack([img, (best_mask.astype(np.uint8) * 255)])\n",
        "out_path = \"/content/sam2_test_cutout.png\"\n",
        "cv2.imwrite(out_path, cv2.cvtColor(rgba, cv2.COLOR_RGBA2BGRA))\n",
        "print(\"Cutout écrit:\", out_path)\n",
        "\n",
        "# 5) visu\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Image + bbox\")\n",
        "plt.imshow(img)\n",
        "x1,y1,x2,y2 = bbox\n",
        "plt.gca().add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1, fill=False, linewidth=2))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Masque (meilleur) + overlay\")\n",
        "overlay = img.copy()\n",
        "overlay[best_mask] = (0.5*overlay[best_mask] + 0.5*np.array([255,255,255])).astype(np.uint8)\n",
        "plt.imshow(overlay)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JdkyQYYViQrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V. Exécution de `Stable Diffusion`"
      ],
      "metadata": {
        "id": "mVSjgR5E9s1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "... *(Ré-insertion **qualitative** de la danseuse dans un nouvel environnement.)*"
      ],
      "metadata": {
        "id": "0Q7oZJBu9v71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "stURjkBl9wmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VI. Conclusion"
      ],
      "metadata": {
        "id": "cP_V5w4K9y4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "SsNLPjrD92XC"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "generative_ai_disabled": true,
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}